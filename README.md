# BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning

üìÑ [Paper](https://arxiv.org/abs/2504.09426) | üåê [Project Page](https://shawnking98.github.io/BabyVLM/)

<img width="100%" src="assets/main_figure_v3.jpg">

[Shengao Wang](https://www.linkedin.com/in/shengao-wang-a259aa173/en)<sup>1</sup>,
[Arjun Chandra](https://www.linkedin.com/in/arjun-chandra2/)<sup>1</sup>,
[Aoming Liu](https://cs-people.bu.edu/amliu/)<sup>1</sup>,
[Venkatesh Saligrama](https://venkatesh-saligrama.github.io/)<sup>1</sup>,
[Boqing Gong](http://boqinggong.info/)<sup>1</sup>

<sup>1</sup>Boston University

## Overview

This is the codebase of BabyVLM evaluation suite, integrated with the [lmms-eval](https://github.com/EvolvingLMMs-Lab/lmms-eval/) framework. Specifically, this repository provides four extra evaluation tasks (*Labeled-S, Visual Two-Word Test, Baby Winoground, SAYCam Caption*) and implementes the model wrapper for BabyLLaVA series in the paper.

## Environment Setup

Install this package by cloning the repository and running the following command:

```bash
git clone https://github.com/ShawnKing98/babylmms-eval.git
cd babylmms-eval
conda create -n babyvlm python=3.10
conda activate babyvlm
pip install -e .
```

Optionally, install the dependencies for BabyLLaVA by following the instructions in the [BabyLLaVA repository](https://github.com/ShawnKing98/BabyLLaVA).

## Data Preparation

The BabyVLM evaluation tasks use data from the [SAYCam](https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00039/97495/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset) dataset, along with our own synthetic data. SAYCam dataset is hosted on the [Databrary](https://nyu.databrary.org/) platform, and we are still seeking an appropriate platform to host our own synthetic data. All the data labels are already included in this repository, yet due to the term of use, we cannot publicly share the images here, interested researchers can apply for access on Databrary with approval from their institution's IRB.

Below are the steps to prepare the data:
- **Acquire SAYCam images**: Instead of directly using the raw SAYCam videos, we use the frames extracted by the authors of [this paper](https://www.science.org/doi/10.1126/science.adi1374). Download the `frames.txt` file from [Databrary](https://nyu.databrary.org/), change the suffix from `.txt` to `.zip`, unzip the file into your local directory, and you should have a folder containing 600,285 images (~14G), call it `path/to/saycam_images/`.
- **Acquire Synthetic data**: The synthetic data is generated by GPT4o and used in the Baby Winoground task. As we are still seeking a platform to host the synthetic data, please [contact us](mailto:wsashawn@bu.edu) to get access to the synthetic data.
- **Post-process**: After acquiring the SAYCam images and synthetic data, you can run the following command to put the images at the right place:

```bash
cd babylmms-eval
ln -s path/to/saycam_images/ dataset/labeled_s/images
ln -s path/to/saycam_images/ dataset/vtwt/images
ln -s path/to/saycam_images/ dataset/SAYCam_caption/images
ln -s path/to/saycam_images/ dataset/baby_winoground/positive_images
ln -s path/to/synthetic_images/ dataset/baby_winoground/negative_images
```

## Usage
In order to evaluate the LLaVA and BabyLLaVA series models, please check our [BabyLLaVA repository](https://github.com/ShawnKing98/BabyLLaVA) to install the necessary dependencies, before running the evaluation. 

**Evaluation of LLaVA-v1.5 on BabyVLM tasks**
```bash
accelerate launch --num_processes=1 -m lmms_eval \
    --model llava \
    --model_args pretrained=liuhaotian/llava-v1.5-7b,conv_template=plain \
    --task vtwt,labeled_s,baby_winoground,saycam_caption \
    --batch_size 16 \
    --output_path ./logs \
    --trust_remote_code
```
**Evaluation of BabyLLaVA on BabyVLM tasks**
```bash
accelerate launch --num_processes=1 -m lmms_eval \
    --model babyllava \
    --model_args pretrained=wsashawn/babyllava_resnext_gpt2,conv_template=plain \
    --task vtwt,labeled_s,baby_winoground,saycam_caption \
    --batch_size 16 \
    --output_path ./logs \
    --trust_remote_code
```
More detail about the usage of this package can be found at the original [lmms-eval repository](https://github.com/EvolvingLMMs-Lab/lmms-eval).

## Add Customized Model
Please refer to the [model guide documentation](docs/model_guide.md) for instructions on how to add your own model. Note that both the `generate_until` and `loglikelihood` methods need to be implemented, as they are both used in the BabyVLM evaluation tasks.

## Citation

Please cite us if you use this repository in your work.

```bibtex
@misc{wang2025babyvlmdataefficientpretrainingvlms,
      title={BabyVLM: Data-Efficient Pretraining of VLMs Inspired by Infant Learning}, 
      author={Shengao Wang and Arjun Chandra and Aoming Liu and Venkatesh Saligrama and Boqing Gong},
      year={2025},
      eprint={2504.09426},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2504.09426}, 
}
```